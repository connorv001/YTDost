from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
from pytube import YouTube
from transformers import pipeline, set_seed
from flask import Flask, jsonify, request
from haystack.document_stores import InMemoryDocumentStore
from haystack.utils import fetch_archive_from_http
from haystack.pipelines.standard_pipelines import TextIndexingPipeline
from haystack.nodes import BM25Retriever, FARMReader
from haystack.pipelines import ExtractiveQAPipeline
import os
from werkzeug.utils import secure_filename
import logging

app = Flask(__name__)
app.config['UPLOAD_EXTENSIONS'] = ['.txt']
app.config['UPLOAD_PATH'] = 'uploads'

def allowed_file(filename):
    return True

@app.route('/summarize', methods=['GET'])
def summarize():
    video_url = request.args.get('url')

    # Extract the video ID from the URL
    video_id = video_url.split("=")[1]

    # Download the video using pytube
    yt = YouTube(video_url)
    stream = yt.streams.filter(file_extension='mp4').first()
    stream.download(output_path='./', filename='video')

    # Retrieve the captions using the YouTube Data API
    youtube = build('youtube', 'v3', developerKey='<API-KEY>')
    captions = youtube.captions().list(videoId=video_id, part='snippet').execute()

    # Extract the transcript text using the YouTube Transcript API
    captions_list = YouTubeTranscriptApi.get_transcript(video_id)
    transcript_text = '\n'.join([c['text'] for c in captions_list])

    # Preprocess the transcript text
    # You may need to add additional preprocessing steps depending on your use case
    transcript_text = transcript_text.replace('\n', ' ')

    # Load the pre-trained Hugging Face model for text generation
    model = pipeline('text2text-generation', model='t5-base', tokenizer='t5-base')

    # Set a random seed for reproducibility
    set_seed(42)

    # Split the transcript text into chunks of maximum sequence length
    max_length = model.tokenizer.max_len_single_sentence
    chunks = [transcript_text[i:i+max_length] for i in range(0, len(transcript_text), max_length)]

    # Generate an essay for each chunk
    essays = []
    for chunk in chunks:
        prompt = f"summarize: {chunk}"
        essay = model(prompt, max_length=256, do_sample=True)[0]['generated_text']
        essays.append(essay)

    # Post-process the essays
    # You may need to add additional post-processing steps depending on your use case
    essay = ' '.join(essays).strip()

    return jsonify({"summary": essay})

@app.route('/predict', methods=['POST'])
def predict():
    if 'file' not in request.files:
        return jsonify({"status": "error", "message": "No file uploaded"})
    file = request.files['file']
    if file.filename == '':
        return jsonify({"status": "error", "message": "No file selected"})
    if not allowed_file(file.filename):
        return jsonify({"status": "error", "message": "Invalid file type"})

    query = request.form.get('query')

    logging.basicConfig(format="%(levelname)s - %(name)s -  %(message)s", level=logging.WARNING)
    logging.getLogger("haystack").setLevel(logging.INFO)

    document_store = InMemoryDocumentStore(use_bm25=True)

    # Index the input file
    file.save(os.path.join(app.config['UPLOAD_PATH'], secure_filename(file.filename)))
    indexing_pipeline = TextIndexingPipeline(document_store)
    indexing_pipeline.run(file_path=os.path.join(app.config['UPLOAD_PATH'], secure_filename(file.filename)))

    # Index the summary file generated by /summarize
    with open('uploads/summary.txt', 'r') as f:
        summary_text = f.read()
    summary_text = summary_text.replace('\n', ' ')
    document_store.write_documents([{"content": summary_text, "meta": {"name": "summary"}}])

    # Retrieve and rank documents using the BM25Retriever
    retriever = BM25Retriever(document_store=document_store)

    # Use the FARMReader to find the answer to the query
    reader = FARMReader(model_name_or_path="deepset/roberta-base-squad2", use_gpu=True)

    # Use the ExtractiveQAPipeline to extract the answer from the document
    pipe = ExtractiveQAPipeline(reader, retriever)

    # Get the top 10 documents and top 5 passages for each document
    prediction = pipe.run(
        query=query,
        params={
            "Retriever": {"top_k": 10},
            "Reader": {"top_k": 5}
        }
    )

    return jsonify(prediction)


if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5002)

